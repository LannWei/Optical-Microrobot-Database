<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Optical Microrobot</title>
    <link rel="stylesheet" type="text/css" href="./static/css/index.css">
    <!-- <link rel="shortcut icon" href="/static/favicon.ico">
    <link rel="icon" href="/static/favicon.png" type="image/png"> -->
</head>

<body>
    <div id="wrap">
        <!-- 顶部 Header 区域 -->
        <header class="header">
            <!-- 左侧 Logo 图片 -->
            <img src="./static/logo.png" alt="Logo" class="logo">

            <!-- 右侧导航链接 -->
            <nav class="nav-links">
                <a href="#intro">Intro</a>
                <a href="#dataset">Dataset</a>
                <a href="#benchmark">Benchmark</a>
                <a href="#download">Download</a>
                <a href="#code">Code</a>
                <a href="#publications">Publications</a>
            </nav>
        </header>

        <!-- 左侧电梯导航 -->
        <nav class="elevator-nav">
            <div class="nav-title">Contents</div>
            <ul class="nav-list">
                <li class="nav-item" data-target="intro">Introduction</li>
                <li class="nav-item" data-target="dataset">Dataset</li>
                <li class="nav-item" data-target="benchmark">Benchmark</li>
                <li class="nav-item" data-target="download">Download</li>
                <li class="nav-item" data-target="code">Code</li>
                <li class="nav-item" data-target="publications">Publications</li>
            </ul>
        </nav>

        <div class="title-section">
            <div class="paper-title">A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose and Depth Perception</div>
            <div class="authors">Lan Wei and Dandan Zhang</div>
            <div class="institution">Imperial College London</div>
            <div class="paper-link">
                <a href="https://ieeexplore.ieee.org/document/11072739" target="_blank">DOI: 10.1109/MARSS65887.2025.11072739</a>
            </div>
        </div>

        <!-- Introduction Section -->
        <div class="section" id="intro">
            <div class="main">
                <h3>Introduction</h3>
                <p>
                    Optical microrobots, manipulated via optical
                    tweezers (OT), have broad applications in biomedicine. However,
                    reliable pose and depth perception remain fundamental
                    challenges due to the transparent, noisy, and dynamic
                    characteristics of the microscale environments in which they
                    operate. An open dataset is crucial for enabling reproducible
                    research, facilitating benchmarking, and accelerating the development
                    of perception models tailored to microscale challenges.
                    Standardized evaluation enables consistent comparison across
                    algorithms, promoting fair assessment and driving progress
                    in the field. Here, we introduce the OpTical MicroRobot
                    dataset (OTMR), the first publicly available dataset designed
                    to support microrobot perception under optical microscope.
                    OTMR contains 232,881 high-resolution images spanning 18
                    microrobot types and 176 distinct poses. We benchmarked
                    the performance of eight deep learning models, including
                    architectures derived via neural architecture search, on two
                    key tasks: pose classification and depth regression. Results
                    indicated that Vision Transformers achieve the highest accuracy
                    in pose classification, while depth regression benefits from
                    deeper architectures. Additionally, increasing the size of the
                    training dataset leads to substantial improvements across both
                    tasks, highlighting OTMR’s potential as a foundational resource
                    for robust and generalisable microrobot perception in complex
                    microscale environments.
                </p>        
            </div>
        </div>

        <!-- Dataset section -->
        <div class="section" id="dataset">
            <div class="main">
                <h3>Dataset</h3>
                <div id="about_content" class="aboutoverview">
                    <!-- III-A How can the microtobot be fabricated? -->
                    <div>
                        <div class="subtitle">How can the microtobot be fabricated?</div>
                        <p>
                            The optical microrobots used in this study were fabricated
                            using a Nanoscribe 3D printer (Nanoscribe GmbH,Germany) with 
                            IP-L 780 photoresist as the printing material.
                            The fabrication process employed two-photon polymerization
                            (2PP). Microrobots were directly printed onto glass substrates 
                            and subsequently immersed in deionized (DI) water within a sealed 
                            spacer chamber for imaging and experimental use.
                        </p>
                        <p style="line-height: 1.45;">
                            Eighteen distinct microrobot designs were fabricated for
                            inclusion in the OTMR dataset. Their CAD models and
                            corresponding focal plane images are shown in Fig. <a href="#fig2" class="a_num">2</a>.
                            Microrobots 1–6 (top row) were primarily used for pose classification,
                            featuring 176 unique out-of-plane poses generated
                            by varying rotation angles from 0<sup>◦</sup> to 90<sup>◦</sup>. All 18 types were
                            used for depth estimation.
                        </p>
                        <figure id="fig2" class="scientific-figure" >
                            <div class="figure-content">
                                <img class="strict-image" src="./resource/dataset-CAD.jpg" alt="Optical Microrobot Visualization">
                            </div>
                            <figcaption class="scientific-caption">
                                Fig. 2. Overview of the 18 microrobot types included in the OTMR dataset. 
                                For each robot, the left image shows its CAD model, and the right image presents the corresponding 
                                experimental image captured at the focus plane under an optical microscope. Microrobots 1–6 (top row) 
                                are specifically designed for the pose classification task due to their varied and distinguishable 
                                orientations, while all 18 types are used for depth estimation tasks.
                            </figcaption>
                        </figure>
                        <figure id="figure*" class="scientific-figure" style="width: 100%;margin-top: 12px;">
                            <div class="image-group">
                                <!-- Gif on the left -->
                                <img src="./resource/test.gif" alt="Git" class="gif-image">
                        
                                <!-- the SVG arrow -->
                                <svg class="arrow" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M3 12h15m0 0l-5-5m5 5l-5 5" stroke="#333" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                </svg>
                        
                                <!-- 6 images on the right -->
                                <div class="result-images">
                                    <img src="./resource/16-Robot-Body-Ball-withLongSpring_OT_00001.jpg" alt="Image 1">
                                    <img src="./resource/16-Robot-Body-Ball-withLongSpring_OT_00010.jpg" alt="Image 2">
                                    <img src="./resource/16-Robot-Body-Ball-withLongSpring_OT_00016.jpg" alt="Image 3">
                                    <img src="./resource/16-Robot-Body-Ball-withLongSpring_OT_00050.jpg" alt="Image 4">
                                    <img src="./resource/16-Robot-Body-Ball-withLongSpring_OT_00066.jpg" alt="Image 5">
                                    <img src="./resource/16-Robot-Body-Ball-withLongSpring_OT_00090.jpg" alt="Image 6">
                                </div>
                            </div>
                            <figcaption class="scientific-caption">Process Optical Tweezer Video to Single Frames</figcaption>
                        </figure>
                        <p>
                            The aboving <a href="#figure*" class="a_num">figure</a> 
                            provides an example video of microrobots recorded using an optical tweezer. For user convenience, 
                            the dataset includes pre-processed frames and labels. To access the original video files, please contact:
                            <a href="mailto:d.zhang17@imperial.ac.uk">d.zhang17@imperial.ac.uk</a> 
                            or <a href="mailto:l.wei24@imperial.ac.uk">l.wei24@imperial.ac.uk</a>.    
                        </p>
                    </div>
                    
                    <!-- III-B How is the video/image collected? -->
                    <div>
                        <div class="subtitle">How is the video/image collected?</div>
                        <p>
                            The data collection system is built around an optical
                            tweezer (OT) platform, exemplified here by the setup from
                            Elliot Scientific (UK), integrated with a nanopositioning
                            stage (Mad City Labs Inc., USA). Microscopic images were
                            captured using a high-speed CCD camera (Basler AG, Germany)
                            mounted on a Nikon Ti microscope with a 100× oil
                            immersion objective. Each image frame has a resolution of
                            678×488 pixels. While this setup is used in our experiments,
                            the data collection process is broadly compatible with other
                            commercial OT systems and optical manipulation platforms
                            that include a high-resolution microscope, making the dataset
                            applicable across a wide range of micro-manipulation research
                            environments. A schematic of the system is shown in
                            Fig. <a href="#fig3" class="a_num">3</a>.
                        </p>
                        <p>
                            During data acquisition, microrobots were fixed to a glass
                            substrate mounted on a piezoelectric stage, enabling precise
                            vertical translation along the z-axis for accurate depth measurements.
                            To generate diverse out-of-plane poses, we fabricated
                            microrobots with systematically varied orientations,
                            which were further manipulated using the piezoelectric drive
                            in either discrete steps or continuous motion. For each pose,
                            over 1,000 image frames were captured to support both pose
                            classification and depth estimation tasks. All data acquisition
                            and processing were conducted offline.
                        </p>
                        <figure id="fig3" class="scientific-figure">
                            <div style="display: flex;">
                                <div>
                                    <div class="figure-content" style="width: 500px;">
                                        <img class="strict-image" src="./resource/OT-SETUP.png" alt="OT-SETUP">
                                    </div>
                                    <figcaption class="scientific-caption">
                                        Fig. 3. Overview of the experimental platform for data collection.
                                    </figcaption>
                                </div>
                                <div style="margin-left: auto;">
                                    <div class="figure-content" style="height:345px; overflow:hidden;">
                                        <img style="object-fit:cover; width:100%; height:100%;" class="strict-image" src="./resource/experimental-setup.jpg" alt="experimental-setup">
                                    </div>
                                    <figcaption class="scientific-caption">
                                        Fig. 4. Image of the experimental setup.
                                    </figcaption>
                                </div>
                            </div>
                        </figure>
                    </div>
                </div>
            </div>
        </div>

        <!-- Benchmark section -->
         <div class="section" id="benchmark">
            <div class="main">
                <h3>Benchmark</h3>
                <p>
                    Table <a href="#table II" class="a_num">II</a> presents the five-fold cross-validation results for
                    pose classification using two different microrobot types, with
                    models trained on an equal number of images per pose.
                    The results indicate that microrobots with more complex
                    structures—such as Robot 3, which incorporates two distinct
                    types of spherical components—pose greater challenges for
                    pose estimation compared to simpler designs like Robot 1,
                    which consists of four identical spheres. For instance, the
                    best pitch and roll prediction accuracies for Robot 3 are 3.4%
                    and 2.7% lower, respectively, than those for Robot 1. Among
                    all evaluated architectures, the Vision Transformer (ViT) consistently
                    outperforms the others across different microrobot
                    types. This superior performance can be attributed to its
                    pretraining on ImageNet, a large-scale classification dataset
                    with over 14 million images, as well as its patch-based image
                    decomposition strategy, which effectively captures local
                    and global features critical for pose recognition in microscale
                    environments. Table <a href="#table III" class="a_num">III</a> compares the computational
                    characteristics of various models for the pose classification
                    task, including the number of parameters (in MB), inference 
                    complexity (in GFLOPs), and real-time processing capability
                    (measured as throughput in images per second). Although
                    the Vision Transformer (ViT) exhibits the highest GFLOPs
                    among all models, it is still capable of processing over
                    1,300 images per second, demonstrating strong real-time
                    performance despite its computational intensity.
                    <figure id="table II" class="scientific-figure" >
                        <div class="table-caption">
                            <b>TABLE II</b><br>
                            POSE CLASSIFICATION FIVE-FOLD CROSS-VALIDATION RESULTS FOR ROBOT 1 AND ROBOT 3 ACROSS ALL MODELS.    
                        </div>
                        <div class="figure-content" >
                            <img class="strict-image" src="./resource/table2.png" alt="table II">
                        </div>
                    </figure>
                    <figure id="table III" class="scientific-figure" >
                        <div class="table-caption">
                            <b>TABLE III</b><br>
                            COMPARISON OF MODEL SIZE (MB), INFERENCE COST (GFLOPS) AND
                            THROUGHPUT THAT MEASURES THE NUMBER OF IMAGES THAT CAN BE
                            PROCESSED PER SECOND FOR POSE CLASSIFICATION ACROSS
                            BENCHMARKED METHODS.   
                        </div>
                        <div class="figure-content" style="width: 500px;">
                            <img class="strict-image" src="./resource/table3.png" alt="table III">
                        </div>
                    </figure>
                </p>

                <p>
                    Table <a href="#table IV" class="a_num">IV</a> summarizes the depth regression results for six
                    different microrobot types. Similar to pose classification,
                    robots with complex and asymmetric geometries (e.g., Robot
                    14) are significantly more difficult to regress accurately. The
                    lowest mean squared error (MSE) obtained on Robot 14
                    is approximately six times higher than that of a simpler
                    design like Robot 8. Furthermore, for a given robot, deeper
                    architectures (e.g., ResNet50) tend to outperform shallower
                    ones (e.g., ResNet18), highlighting the need for higher model
                    capacity in depth estimation tasks.
                    <figure id="table IV" class="scientific-figure" >
                        <div class="table-caption">
                            <b>TABLE IV</b><br>
                            DEPTH REGRESSION RESULTS FOR ROBOTS 8-18 ACROSS ALL MODELS.   
                        </div>
                        <div class="figure-content" >
                            <img class="strict-image" src="./resource/table4.png" alt="table IV">
                        </div>
                    </figure>
                </p>

                <p>
                    The results of neural architecture search (NAS) are shown
                    in Table <a href="#table V" class="a_num">V</a>. The NAS process was applied to CNN-based architectures
                    and trained from scratch. As reported in Tables <a href="#table II" class="a_num">II</a>
                    and <a href="#table IV" class="a_num">IV</a>, the NAS-optimized models consistently outperform
                    the baseline CNN in all evaluated cases. Notably, the NAS
                    model achieved the best depth regression performance on
                    Robot 16, one of the most complex designs, demonstrating
                    the effectiveness of architecture search in tailoring models
                    to task difficulty. Moreover, the architectures discovered by
                    NAS further reflect the relative difficulty of the tasks. The
                    optimal model for depth regression includes two additional
                    convolutional layers and a larger fully connected layer compared
                    to the model optimized for pose classification, aligning
                    with the inherently more complex nature of continuous-value
                    prediction in regression tasks.
                    <figure id="table V" class="scientific-figure" >
                        <div class="table-caption">
                            <b>TABLE V</b><br>
                            OPTIMAL ARCHITECTURES FOUND BY NAS FOR POSE CLASSIFICATION AND DEPTH REGRESSION. 
                        </div>
                        <div class="figure-content" style="width: 400px;">
                            <img class="strict-image" src="./resource/table5.png" alt="table V">
                        </div>
                    </figure>
                </p>

                <!-- paper-IV-D: Transfer Learning Among Different Robots -->
                <div>
                    <div class="subtitle">paper-IV-D: Transfer Learning Among Different Robots</div>
                    <p>
                        To evaluate the generalisation ability of deep learning
                        models, we conducted a transfer learning experiment using
                        the best-performing model—ViT—trained on data from
                        Robot Type 3 for the pose classification task. The trained
                        model was directly tested on Robot Types 1, 4, and 5 without
                        further fine-tuning. The evaluation metric is the average
                        classification accuracy of pitch and roll angles.
                    </p>
                    <p>
                        As shown in Fig. <a href="#fig7" class="a_num">7</a>, the model achieves the highest
                        accuracy on Robot 3, the training target, as expected. Robots
                        4 and 5 exhibit higher classification accuracy than Robot 1,
                        likely due to structural similarity to Robot 3. All three robots
                        (3, 4, and 5) share a common feature: they are composed
                        of two distinct types of spherical components along the
                        arms. In contrast, Robot 1 consists of four identical spheres,
                        which differ significantly in geometry and visual features.
                        Furthermore, the spatial orientation of the robot also affects
                        transfer performance. Robot 5 shares the same horizontal
                        configuration as Robot 3, leading to better generalisation and
                        higher accuracy. In contrast, Robot 4 is vertically oriented,
                        which introduces a distribution shift in visual appearance
                        and results in reduced classification performance compared
                        to Robot 5.
                    </p>
                    <figure id="fig7" class="scientific-figure" >
                        <div class="figure-content" style="width: 550px;">
                            <img class="strict-image" src="./resource/figure7.png" alt="figure 7">
                        </div>
                        <figcaption class="scientific-caption">
                            Fig. 7. Transfer learning results of the ViT model trained on Robot Type
                            3 and tested on different robot types without fine-tuning. The evaluation
                            metric is the average classification accuracy of pitch and roll angles.
                        </figcaption>
                    </figure>
                </div>

                <!-- paper-IV-E: Model Interpretability -->
                <div>
                    <div class="subtitle">paper-IV-E: Model Interpretability</div>
                    <p>
                        To gain insights into which regions of an input microrobot
                        image influence the model’s predictions, we employ
                        Gradient-weighted Class Activation Mapping (Grad-
                        CAM) to visualize the spatial attention of the CNN during 
                        pose classification. As illustrated in Fig. <a href="#fig8" class="a_num">8</a>, the Grad-
                        CAM heatmaps highlight the areas that contribute most to
                        the model’s decision-making process.
                    </p>
                    <p>
                        The visualizations for Robot Types 1 and 3 reveal that the
                        model consistently attends to the microrobot structure itself,
                        particularly the arms and spherical components when determining
                        the pose. This confirms that CNN has successfully
                        learned to focus on the relevant features of the microrobot
                        rather than background noise, providing interpretability and
                        confidence in the model’s classification behaviour.
                    </p>
                    <figure id="fig8" class="scientific-figure" >
                        <div class="figure-content" style="width: 550px;">
                            <img class="strict-image" src="./resource/figure8.png" alt="figure 8">
                        </div>
                        <figcaption class="scientific-caption">
                            Fig. 8. Grad-CAM visualizations for pose classification on Robot Types
                            1 and 3 using a CNN model. Each pair shows the original microscope
                            image (left) and its corresponding Grad-CAM heatmap (right). The red
                            regions indicate high-importance areas that the model relies on most for
                            its predictions, while blue regions indicate areas of low importance that are
                            largely ignored.
                        </figcaption>
                    </figure>
                </div>
                
                <!-- paper-IV-F: Influence of Data Size -->
                <div>
                    <div class="subtitle">paper-IV-F: Influence of Data Size</div>
                    <p>
                        To evaluate the impact of dataset size on depth regression
                        performance, we conduct experiments using Robot Type
                        8 and the best-performing model identified in Table <a href="#table IV" class="a_num">IV</a>,
                        ResNet50, trained for 10 epochs. The complete dataset
                        for this robot consists of 5,600 images. We train and test
                        the model using varying proportions of the data: 100%,
                        80%, 60%, 40%, and 20%, while maintaining a fixed
                        train/validation/test split of 8:1:1 in each case. As shown
                        in Fig. <a href="#fig9" class="a_num">9</a>, increasing the amount of training data consistently
                        reduces the mean squared error (MSE) and improves the
                        R2 score, indicating better regression accuracy and stronger
                        predictive reliability. These results emphasize the importance
                        of large-scale data availability for training deep learning models 
                        in micro-scale environments and demonstrate the
                        value of the OTMR dataset in enabling robust, data-driven
                        depth estimation.
                    </p>
                    <figure id="fig9" class="scientific-figure" >
                        <div class="figure-content" style="width: 450px;">
                            <img class="strict-image" src="./resource/figure9.png" alt="figure 9">
                        </div>
                        <figcaption class="scientific-caption">
                            Fig. 9. Impact of training data size on depth regression performance using
                            ResNet50 for Robot Type 8.
                        </figcaption>
                    </figure>
                </div>
            </div>
         </div>

        <!-- Download Section -->
        <div class="section" id="download">
            <div class="main">
                <h3>Download</h3>
                <div id="about_content" class="aboutoverview">
                    <!-- <p>The optical microrobot database is available at (Data will be released after the paper is accepted):</p> -->
                    <p>The optical microrobot database is available at:</p>
                    <div style="text-align: center;">
                        <a href="https://huggingface.co/datasets/Lan-2025/OpticalMicrorobot/tree/main" class="google-device-link" target="_blank">
                            <svg style="transform: scale(1.3);margin-right: 5px;" class="mr-1 text-gray-400 group/repo-hover:text-red-500 flex-none -ml-0.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
                            <span>Optical Microrobot Dataset</span>
                        </a>   
                    </div>      
                    <p>
                    If you encounter any issues accessing the files, please feel free to
                    contact us at <a href="mailto:d.zhang17@imperial.ac.uk">d.zhang17@imperial.ac.uk</a> or <a href="mailto:l.wei24@imperial.ac.uk">l.wei24@imperial.ac.uk</a>.
                    </p>
                    <p><strong>Citation:</strong> When using the datasets, please cite: L. Wei and D. Zhang, "A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose and Depth Perception," 2025 International Conference on Manipulation, Automation and Robotics at Small Scales (MARSS), West Lafayette, IN, USA, 2025, pp. 1-8, doi: 10.1109/MARSS65887.2025.11072739.</p>
                </div>
            </div>
        </div>

        <!-- Code Section -->
        <div class="section" id="code">
            <div class="main">
                <h3>Code</h3>
                <p class="intro-text">
                    <!-- Codes will be released after the paper is accepted. -->
                    All experimental implementations are available at:
                </p>
                <div style="text-align: center;">
                    <a href="https://github.com/LannWei/Optical-Microrobot-Database" class="github-link" target="_blank" rel="noopener noreferrer">
                        <span class="github-icon"></span>
                        LannWei / Optical-Microrobot-Database
                    </a>
                </div>
            </div>
        </div>

        <!-- Publications Section  -->
        <div class="section" id="publications">
            <div class="main">
                <h3>Publications</h3>
                <div id="about_content" class="aboutoverview">
                    <ol>
                        <li>Zhang, Dandan, Antoine Barbot, Florent Seichepine, Frank P-W. Lo,
                            Wenjia Bai, Guang-Zhong Yang, and Benny Lo. "<strong>Micro-object pose
                            estimation with sim-to-real transfer learning using small
                            database.</strong>" Communications Physics 5, no. 1 (2022): 80.</li>
                        <li>Zhang, Dandan, Yunxiao Ren, Antoine Barbot, Florent Seichepine,
                            Benny Lo, Zhuo-Chen Ma, and Guang-Zhong Yang. "<strong>Fabrication and
                            optical manipulation of micro-robots for biomedical
                            applications.</strong>" Matter 5, no. 10 (2022): 3135-3160.</li>
                        <li>Zhang, Dandan, Antoine Barbot, Florent Seichepine, Frank P-W. Lo,
                            Wenjia Bai, Guang-Zhong Yang, and Benny Lo. "<strong>Micro-object pose
                            estimation with sim-to-real transfer learning using small
                            dataset.</strong>" Communications Physics 5, no. 1 (2022): 80.</li>
                        <li>Ren, Yunxiao, Meysam Keshavarz, Salzitsa Anastasova, Ghazal
                            Hatami, Benny Lo, and Dandan Zhang. "<strong>Machine learning-based realtime
                            localization and automatic trapping of multiple microrobots in
                            optical tweezer.</strong>" In 2022 international conference on manipulation,
                            automation and robotics at small scales (MARSS), pp. 1-6. IEEE, 2022.</li>
                        <li>Zhang, Dandan, Frank P-W. Lo, Jian-Qing Zheng, Wenjia Bai, Guang-
                            Zhong Yang, and Benny Lo. "<strong>Data-driven microscopic pose and
                            depth estimation for optical microrobot manipulation.</strong>" Acs
                            Photonics 7, no. 11 (2020): 3003-3014.</li>
                        <li>Zhang, Dandan, Antoine Barbot, Benny Lo, and Guang‐Zhong Yang.
                            "<strong>Distributed force control for microrobot manipulation via planar
                            multi‐spot optical tweezer.</strong>" Advanced Optical Materials 8, no. 21
                            (2020): 2000543</li>
                    </ol>
                </div>
            </div>
        </div>

        <div class="note-box">
            <div class="note-title">
                <span class="note-icon"></span>
                Note
            </div>
            <div class="note-content">
                The open-source resources—including the dataset, deep learning models, benchmarking results, and 
                documentation will be continuously updated. The presented algorithms are generalisable and can be 
                applied to other types of microscopic imaging data for perception-related tasks. In future work, 
                we plan to integrate more advanced AI techniques to further enhance benchmarking capabilities and 
                expand the applicability of our platform.
            </div>
        </div>

        <div id="footer">
            <p> &copy; 2024 <a href="https://www.intelligentrobotics-acrossscales.com/">Multi-Scale Embodied Intelligence Lab, </a>
                <a href="https://www.imperial.ac.uk/">Imperial College London</a>, 
                <a href="mailto:d.zhang17@imperial.ac.uk">d.zhang17@imperial.ac.uk</a> &
                <a href="mailto:l.wei24@imperial.ac.uk">l.wei24@imperial.ac.uk</a> 
            </p>
        </div>
    </div>
</body>

<script>
const navItems = document.querySelectorAll('.nav-item');
const sections = document.querySelectorAll('.section');

// 滚动事件的处理逻辑
function scrolling() {
    let currentSection = '';
    
    sections.forEach(section => {
        const sectionTop = section.offsetTop;
        const sectionHeight = section.clientHeight;
        
        if (window.scrollY >= sectionTop - 25 && 
            window.scrollY < sectionTop + sectionHeight - 25) {
            currentSection = section.id;
        }
    });

    navItems.forEach(item => {
        item.classList.remove('active');
        if (item.getAttribute('data-target') === currentSection) {
            item.classList.add('active');
        }
    });
}

// 使用requireAnimationFrame控制滚动事件的频率
let ticking = false;
let scrollAnimationFrameId;
const bindedFunc = () => {
    if (!ticking) {
        scrollAnimationFrameId = window.requestAnimationFrame(() => {
            scrolling();
            ticking = false;
        });
        ticking = true;
    }
};

// 顶部navigation 点击事件
document.querySelectorAll('.nav-links a').forEach(anchor => {
    anchor.addEventListener('click', function(e) {
        e.preventDefault(); // 阻止默认跳转
        window.removeEventListener('scroll', bindedFunc);
        cancelAnimationFrame(scrollAnimationFrameId); 
        
        const targetId = this.getAttribute('href');
        const targetElement = document.querySelector(targetId);
        
        // 平滑滚动到目标位置
        targetElement.scrollIntoView({
            behavior: 'smooth',
            block: 'start' // 对齐到目标顶部
        });
        
        // 可选：更新URL（不刷新页面）
        history.pushState(null, null, targetId);

        navItems.forEach(item => {
            item.classList.remove('active');
            if ("#" + item.getAttribute('data-target') === targetId) {
                item.classList.add('active');
            }
        });
    });
});

// 侧边navigation 点击事件
navItems.forEach(item => {
    item.addEventListener('click', function() {
        window.removeEventListener('scroll', bindedFunc);
        cancelAnimationFrame(scrollAnimationFrameId); 

        const targetId = this.getAttribute('data-target');
        const targetSection = document.getElementById(targetId);

        let tmp = document.querySelector('.elevator-nav .active')
        if (tmp) tmp.className = 'nav-item';
        this.classList.add('active');
        
        window.scrollTo({
            top: targetSection.offsetTop - 20,
            behavior: 'smooth'
        });
    });
});

// 鼠标滚轮 => 添加scroll事件监听
window.addEventListener('wheel', function() {
    window.addEventListener('scroll', bindedFunc)
})
// 滚动时高亮当前区域对应的导航项
window.addEventListener('scroll', bindedFunc);
// 初始化时触发一次滚动检测
window.dispatchEvent(new Event('scroll'));

// ===========================================================================
// 当窗口大小变化时，隐藏/显示电梯导航
function handleResize() {
    const nav = document.querySelector('.elevator-nav');
    
    if (!nav) return;
    if (window.innerWidth < 1300) {
        nav.style.display = 'none';
    } else {
        nav.style.display = 'block';
    }
}

// 初始化执行
handleResize();
// 监听窗口变化
let resizeTicking = false;
window.addEventListener('resize', () => {
    if (!resizeTicking) {
        requestAnimationFrame(() => {
            handleResize();
            resizeTicking = false;
        });
        resizeTicking = true;
    }
});
</script>

</html>
